{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f75d2cf",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b21b1d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9632cc",
   "metadata": {},
   "source": [
    "### RMSNorm\n",
    "- Standard LayerNorm does two things - centers the data then scales by the standard deviation.  \n",
    "- RMSNorm simplifies this by skipping the mean centering and just normalizes by the root mean square of the values.\n",
    "- It is cheaper in temrs of compute and just as effective emperically.  \n",
    "$$\\text{RMS}(x) = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n} x_i^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45079cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init(self, emb_dim, eps=1e-6, bias=False, qwen3_compatible=True):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.qwen3_compatible = qwen3_compatible\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeroes(emb_dim)) if bias else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        input_dtype = x.dtype\n",
    "\n",
    "        if self.qwen3_compatible:\n",
    "            x = x.to(torch.float32)\n",
    "\n",
    "        variance = x.pow(2).mean(dim=-1, keepdim=True)\n",
    "        norm_x = x * torch.rsqrt(variance + self.eps)\n",
    "        norm_x = norm_x * self.scale\n",
    "\n",
    "        if self.shift is not None:\n",
    "            norm_x = norm_x + self.shift\n",
    "        \n",
    "        return norm_x.to(input_dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a5e52f",
   "metadata": {},
   "source": [
    "### ROPE\n",
    "- Transformers process tokens in parallel, so they have no inherent information about token positions.\n",
    "- Old GPT does this via learned position params. But this causes position info to degrade through the layers.\n",
    "- RoPE encodes absolute position information of a token as rotation applied to its query and key vectors. When the dot product between Q and K is computed, the result naturally depends on the relative distance between the two tokens.\n",
    "<div align=\"center\">\n",
    "   <img src=\"./assets/rope.png\" alt=\"RoPE Illustration\" width=\"350px\"/><br>\n",
    "   <sub><b>Figure:</b> A gentle introduction to Rotary Position Embedding (<a href=\"https://krasserm.github.io/2022/12/13/rotary-position-embedding/\">image source</a>)</sub>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd5253fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rope_params(head_dim, theta_base=10_000, context_length=4096, dtype=torch.float32):\n",
    "    assert head_dim % 2 == 0, \"Embedding dim must be even\"\n",
    "\n",
    "    # Compute the inverse frequencies\n",
    "    inv_freq = 1.0 / (theta_base ** (torch.arange(0, head_dim, 2, dtype=dtype)[: (head_dim // 2)].float() / head_dim))\n",
    "\n",
    "    # Generate position indices\n",
    "    positions = torch.arange(context_length, dtype=dtype)\n",
    "\n",
    "    # Compute the angles\n",
    "    angles = positions.unsqueeze(1) * inv_freq.unsqueeze(0) # Shape: (context_length, head // 2)\n",
    "\n",
    "    # Expand angles to match the head_dim\n",
    "    angles = torch.cat([angles, angles], dim=1) # Shape: (context_length, head_dim)\n",
    "\n",
    "    # Precompute sin and cos\n",
    "    cos = torch.cos(angles)\n",
    "    sin = torch.sin(angles)\n",
    "\n",
    "    return cos, sin\n",
    "\n",
    "\n",
    "def apply_rope(x, cos, sin):\n",
    "    # x : (batch-size, num_heads, seq_len, head_dim)\n",
    "    batch_size, num_heads, seq_len, head_dim = x.shape\n",
    "    assert head_dim % 2 == 0, \"Head dim must be even\"\n",
    "\n",
    "    # Split x into first half and second half\n",
    "    x1 = x[..., : head_dim // 2] # First half\n",
    "    x2 = x[..., head_dim // 2 :] # Second half\n",
    "\n",
    "    # Adjust sin and cos shapes\n",
    "    cos = cos[:seq_len, :].unsqueeze(0).unsqueeze(0) # Shape: (1, 1, seq_len, head_dim)\n",
    "    sin = sin[:seq_len, :].unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    # Apply the rotations\n",
    "    rotated = torch.cat((-x2, x1), dim=-1)\n",
    "    x_rotated = (x * cos) + (rotated * sin)\n",
    "\n",
    "    # It's ok to use lower-precision after applying cos and sin rotation\n",
    "    return x_rotated.to(dtype=x.dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f77947",
   "metadata": {},
   "source": [
    "### FeedForward\n",
    "**Standard MLP**\n",
    "- There are two linear layers. \n",
    "- The input vector is first projected out into a higher dimension and then a ReLU activation is applied on that intermediate vector. ReLU hard suppresses negative values to 0. \n",
    "- Finally the intermediate vector is projected back to the original input dimension.\n",
    "\n",
    "**SwiGLU in Qwen**\n",
    "- There are three linear layers. \n",
    "- The input vector is projected out independently into two higher dimension intermediate vectors - the candidate and the gate. \n",
    "- SilU (x * sigmoid(x)) is applied on the gate and that product is multiplied with the candidate. \n",
    "- SilU unlike ReLU allows for negative values to exist as rather small negative values than 0. \n",
    "- Essentially, this creates a gating mechanism whereby the features of the candidate vector are weighted by the gating vector (element-wise multiplication). \n",
    "- Finally their product vector is projected back into the input dimension.\n",
    "  \n",
    "<div align=\"center\">\n",
    "   <img src=\"./assets/activation.png\" alt=\"Activations\" width=\"350px\"/><br>\n",
    "   <sub><b>Figure:</b> SiLU vs ReLU</sub>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ddbada5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(cfg[\"emb_dim\"], cfg[\"hidden_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
    "        self.fc2 = nn.Linear(cfg[\"emb_dim\"], cfg[\"hidden_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
    "        self.fc3 = nn.Linear(cfg[\"hidden_dim\"], cfg[\"emb_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_fc1 = self.fc1(x)\n",
    "        x_fc2 = self.fc2(x)\n",
    "        x = nn.functional.silu(x_fc1) * x_fc2\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408ed10f",
   "metadata": {},
   "source": [
    "### GQA\n",
    "**Self Attention - Scaled Dot Product**\n",
    "1. Initialize QKV matices by muliplying input embedding vector with weights\n",
    "2. Dot product QK to get unnormalized attention weights\n",
    "3. Scale the unnormalized attention weights (divide by root of the dim of k)\n",
    "4. Normalize using softmax\n",
    "5. Finally multiply the normalized attention weights by V to obtain the context vector\n",
    "<div align=\"center\">\n",
    "   <img src=\"./assets/self-attention.webp\" alt=\"Self Attention\" width=\"350px\"/><br>\n",
    "   <sub><b>Figure:</b> Self-Attention for a token that is second in a sequence (<a href=\"https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention\">image source</a>)</sub>\n",
    "</div>\n",
    "\n",
    "**Adding Causal Masking and MHA will get you to the original Attention is All You Need Paper**\n",
    "\n",
    "Unlike MHA, where each head also has its own set of keys and values, to reduce memory usage, GQA groups multiple heads to share the same key and value projections.\n",
    "<div align=\"center\">\n",
    "   <img src=\"./assets/mha-gqa.webp\" alt=\"MHA vs. GQA\" width=\"350px\"/><br>\n",
    "   <sub><b>Figure:</b> MHA vs. GQA (<a href=\"https://github.com/rasbt/LLMs-from-scratch/blob/main/ch04/04_gqa/README.md\">image source</a>)</sub>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5b073a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
